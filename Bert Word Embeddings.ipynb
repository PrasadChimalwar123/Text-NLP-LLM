{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90b9db233ce4a9587543815d268ae98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chima\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\chima\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c339fbaad74e4e9e8d5054768e79c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc06f42967aa42eda090f68c0b3e0162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fea48b606241d6ab59f5bf6ed113fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dde5d61661f46fbb138a7f50df21033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[ 101, 1045, 2081, 1037, 4086, 2796, 5785, 9841, 2170, 1047, 5886,  102]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Token Type IDs: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Tokens: ['[CLS]', 'i', 'made', 'a', 'sweet', 'indian', 'rice', 'dish', 'called', 'k', '##her', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"I made a sweet indian rice dish called kher.\"\n",
    "\n",
    "# Tokenize with BERT tokenizer\n",
    "bert_inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(\"Token IDs:\", bert_inputs['input_ids'])\n",
    "\n",
    "attention_mask = bert_inputs['attention_mask']\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "\n",
    "token_type_ids = bert_inputs['token_type_ids']\n",
    "print(\"Token Type IDs:\", token_type_ids)\n",
    "\n",
    "# Print the tokens themselves to understand the splits\n",
    "tokens = tokenizer.convert_ids_to_tokens(bert_inputs['input_ids'][0])\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text\n",
    "text = \"I made a sweet indian rice dish called kher.\"\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last hidden state (embeddings)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the last hidden state (embeddings): torch.Size([1, 13, 768])\n"
     ]
    }
   ],
   "source": [
    "# Print the dimensions of the embeddings\n",
    "print(\"Shape of the last hidden state (embeddings):\", last_hidden_states.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS], Embedding Dimension: torch.Size([768]), Embedding (first 5 components): tensor([-3.9467e-01, -1.6321e-04, -3.2087e-01,  1.0070e-01, -7.9335e-02])...\n",
      "Token: i, Embedding Dimension: torch.Size([768]), Embedding (first 5 components): tensor([ 0.6472, -0.3602, -0.3613,  0.1264, -0.0192])...\n",
      "Token: made, Embedding Dimension: torch.Size([768]), Embedding (first 5 components): tensor([-0.0117,  0.2325, -0.5741, -0.1633,  0.3904])...\n",
      "Token: a, Embedding Dimension: torch.Size([768]), Embedding (first 5 components): tensor([-0.3974, -0.1173, -0.7911,  0.2142,  0.8628])...\n",
      "Token: sweet, Embedding Dimension: torch.Size([768]), Embedding (first 5 components): tensor([-0.2430, -0.1461, -0.1745,  0.0477,  0.4450])...\n",
      "Token: indian, Embedding Dimension: torch.Size([768]), Embedding (first 5 components): tensor([-0.0251,  0.0099, -0.7872, -0.3906,  0.5776])...\n",
      "Token: rice, Embedding Dimension: torch.Size([768]), Embedding (first 5 components): tensor([ 0.3817,  0.4865, -0.0401, -0.0100, -0.2390])...\n",
      "Token: dish, Embedding Dimension: torch.Size([768]), Embedding (first 5 components): tensor([ 0.1931, -0.1638, -0.3596, -0.0629,  0.0434])...\n",
      "Token: called, Embedding Dimension: torch.Size([768]), Embedding (first 5 components): tensor([-0.3786, -0.2023, -0.3983,  0.2099,  0.3202])...\n",
      "Token: k, Embedding Dimension: torch.Size([768]), Embedding (first 5 components): tensor([ 0.4694, -0.8502, -0.3339, -1.0939, -0.3265])...\n",
      "Token: ##her, Embedding Dimension: torch.Size([768]), Embedding (first 5 components): tensor([ 0.4137, -0.1786, -0.7588, -0.7533,  0.8160])...\n",
      "Token: ., Embedding Dimension: torch.Size([768]), Embedding (first 5 components): tensor([ 0.3976,  0.0440, -0.3237,  0.4922, -0.1690])...\n",
      "Token: [SEP], Embedding Dimension: torch.Size([768]), Embedding (first 5 components): tensor([ 0.5299,  0.1581, -0.1627,  0.6848, -0.1899])...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print embeddings for each token along with their vector dimension\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "for token, embedding in zip(tokens, last_hidden_states[0]):\n",
    "    print(f\"Token: {token}, Embedding Dimension: {embedding.shape}, Embedding (first 5 components): {embedding[:5]}...\")  # Display first 5 components for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: The whole idea of converting words into arrays of numbers is fascinating. These are context-aware embeddings, meaning that each array for a given word can look different depending on its placement and meaning in the sentence. It's truly amazing, these embeddings are further used in the decoder to predict the next words!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
